# PufferLib training config for AuctionDraftEnv
# Inherits defaults from pufferlib/config/default.ini where not overridden.
# Use with scripts/train_puffer.py --curriculum-phase {1,2,3}

[base]
env_name = auction_draft

[train]
# Hardware
device = cpu
precision = float32
cpu_offload = False
torch_deterministic = True
seed = 42

# Optimizer
optimizer = adam
learning_rate = 3e-4
adam_beta1 = 0.9
adam_beta2 = 0.999
adam_eps = 1e-8
anneal_lr = True
min_lr_ratio = 0.1

# Batch sizing
# Episodes are ~20-40 steps long; bptt_horizon > episode length is fine —
# PufferLib handles episode boundaries via the terminals flag.
#
# IMPORTANT: segments = batch_size // bptt_horizon must be divisible by num_envs
# for every phase. Phase configs use num_envs ∈ {1, 4, 6}, LCM = 12.
# 3072 // 64 = 48; 48 % 12 = 0 ✓. Also 3072 % 512 (minibatch_size) = 0 ✓.
batch_size = 3072
bptt_horizon = 64
minibatch_size = 512
max_minibatch_size = 2048

# PPO hyperparameters
update_epochs = 4
gamma = 0.99
gae_lambda = 0.95
clip_coef = 0.2
vf_coef = 0.5
vf_clip_coef = 0.2
ent_coef = 0.01
max_grad_norm = 0.5

# V-trace (importance sampling) — set to large values to disable clipping
vtrace_rho_clip = 5.0
vtrace_c_clip = 5.0

# Priority experience replay
prio_alpha = 0.6
prio_beta0 = 0.4

# Curriculum phases (overridden per phase in train_puffer.py)
# Phase 1: draft-only warm-up
total_timesteps = 100_000

# Checkpointing / logging
data_dir = checkpoints/puffer
checkpoint_interval = 50
name = auction_draft
project = ffai

# Torch compile (off for CPU)
compile = False
compile_mode = default
compile_fullgraph = False

# RNN
use_rnn = False

[self_play]
# Phase 4 self-play configuration
# Fraction of opponent slots that use heuristic bidding (vs. learned checkpoints)
heuristic_fraction = 0.3
# Maximum number of past checkpoints to retain in the pool
max_pool_size = 10
# How many training steps between policy snapshots added to the pool
checkpoint_interval_steps = 50_000
# Behavioral cloning auxiliary reward weight (0 = disabled)
bc_alpha = 0.05
